import { promises as fs } from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const rootDir = path.join(__dirname, '..')

const SOURCE_CSV = path.join(rootDir, 'src/data/wolf-context/freqrnc2011.csv')
const OUTPUT_DIR = path.join(rootDir, 'src/data/wolf-context')
const MAX_WORDS = 50000
const CHUNK_SIZE = 2000

function normalizeWord(value) {
  return value
    .toLowerCase()
    .trim()
}

function isValidWord(word) {
  if (word.length < 3 || word.length > 15) return false
  if (!/^[а-яё]+$/u.test(word)) return false
  return true
}

async function ensureOutputDir() {
  await fs.mkdir(OUTPUT_DIR, { recursive: true })
}

async function removeOldChunks() {
  const entries = await fs.readdir(OUTPUT_DIR)
  const targets = entries.filter((name) => /^words-chunk-\d+\.json$/.test(name))
  await Promise.all(targets.map((name) => fs.unlink(path.join(OUTPUT_DIR, name))))
}

async function readCsv() {
  const raw = await fs.readFile(SOURCE_CSV, 'utf8')
  const lines = raw.split(/\r?\n/).filter(Boolean)
  lines.shift() // remove header
  const freqMap = new Map()

  for (const line of lines) {
    const parts = line.split('\t')
    if (parts.length < 3) continue
    const lemma = normalizeWord(parts[0])
    const freq = parseFloat(parts[2])
    if (Number.isNaN(freq)) continue
    if (!isValidWord(lemma)) continue
    const current = freqMap.get(lemma)
    if (current === undefined || freq > current) {
      freqMap.set(lemma, freq)
    }
  }

  return Array.from(freqMap.entries())
}

function chunkArray(arr, size) {
  const chunks = []
  for (let i = 0; i < arr.length; i += size) {
    chunks.push(arr.slice(i, i + size))
  }
  return chunks
}

async function writeChunks(words) {
  const chunks = chunkArray(words, CHUNK_SIZE)
  await Promise.all(
    chunks.map((chunk, index) => {
      const fileName = `words-chunk-${index + 1}.json`
      const filePath = path.join(OUTPUT_DIR, fileName)
      return fs.writeFile(filePath, JSON.stringify(chunk, null, 2), 'utf8')
    })
  )
  return chunks.length
}

async function writeDictionaryTs(chunkCount) {
  const imports = []
  const spreads = []
  for (let i = 1; i <= chunkCount; i += 1) {
    const name = `chunk${i}`
    imports.push(`import ${name} from './words-chunk-${i}.json'`)
    spreads.push(`  ...${name}`)
  }

  const content = `// Auto-generated by scripts/build-wolf-dictionary.mts\n${imports.join(
    '\n'
  )}\n\nexport const wolfDictionary: string[] = [\n${spreads.join(',\n')}\n]\n`
  await fs.writeFile(path.join(OUTPUT_DIR, 'dictionary.ts'), content, 'utf8')
}

async function main() {
  await ensureOutputDir()
  await removeOldChunks()
  const entries = await readCsv()
  const sorted = entries.sort((a, b) => b[1] - a[1]).slice(0, MAX_WORDS)
  const words = sorted.map(([word]) => word)
  const chunkCount = await writeChunks(words)
  await writeDictionaryTs(chunkCount)

  console.log(`Слов взято: ${words.length}`)
  console.log(`Создано чанков: ${chunkCount}`)
}

main().catch((error) => {
  console.error('Ошибка генерации словаря волка', error)
  process.exit(1)
})
